{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5f36de4-5ff2-4207-b23d-1237309dc8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\anaconda3\\envs\\Phi-3-mini\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from modelscope.utils.hf_util import AutoModelForCausalLM, AutoTokenizer,snapshot_download\n",
    "\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "\n",
    "download_path = \"D:\\\\Jupyter_Code\\\\model\"\n",
    "start = 0\n",
    "standard_answer = \"task.csv\"\n",
    "model_dir_list = [\"D:\\\\Jupyter_Code\\\\model\\\\Yi-34B-Chat-4bits\", \n",
    "                  \"D:\\Jupyter_Code\\model\\LLM-Research\\\\Llama-3.2-3B\",\n",
    "                  \"D:\\\\Jupyter_Code\\model\\\\Phi-3-mini-4k-instruct\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4498aeae-4ba8-43fa-af7f-d4f005caf2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Yi_text_generation(input, model_index = 1, max_new_tokens = 230, temperature = 0.3, top_k = 3, do_sample = True, print = True):\n",
    "    '''\n",
    "    We package this LLM into one function. Just call this function, and it will give the result(response).\n",
    "    Models have been installed in D:\\Jupyter_Code\\model. \n",
    "    There are three models: Yi-6B, Yi-34B(cannnot run in this computer), and Yi-34B-Chat-4bits.\n",
    "    The format of input depends on your mode. If mode is 0, input should be a str which the model will generate the text after it;\n",
    "    if mode is 1, input should be the format of chat. We highly recommend you use mode 0.\n",
    "    Other parameters are used to give LLM. Details can be seen at https://zhuanlan.zhihu.com/p/653926703.\n",
    "    Parameters maybe not important when using GPT-4, but it is important when using open-source LLM. \n",
    "    The best parameter depends on your task. You may try some times to get the optimum.\n",
    "    To make it more convenient to use, we give the default parameters. You can set it when you use this function.\n",
    "\n",
    "    我们将调用大语言模型的任务封装在这个函数里。只需要调用这个函数，就会给出返回结果。\n",
    "    需要调用的模型已被安装在D:\\Jupyter_Code\\model文件夹下。目前文件夹内有三个模型：Yi-6B, Yi-34B（本电脑显卡配置无法运行此模型），Yi-34B-Chat-4bits。\n",
    "    输入的参数input格式与mode有关。如果mode=0，您需要传入待补全的字符串，模型进行文本补全任务；如果mode=1，您需要传入待生成的对话，模型进行对话任务。\n",
    "    我们强烈您尽可能使用文本生成工作（即mode=0）。\n",
    "    其他参数是用于给大语言模型的，详细内容可以在这个知乎专栏看到https://zhuanlan.zhihu.com/p/653926703。\n",
    "    参数可能对于使用GPT-4并不重要，但使用开源大模型时是不可或缺的重要一环。最佳参数与任务有关，您可能需要尝试多次才能找到最合适的结果。\n",
    "    为了更方便地使用它，我们有预设的参数。您可以在使用该模型时更改它。\n",
    "    '''\n",
    "    global start, model_dir, model, tokenizer, model_dir_list\n",
    "     \n",
    "    if start == 0:\n",
    "        start = 1\n",
    "        model_dir = model_dir_list[model_index]\n",
    "        if model_index == 0:\n",
    "\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"cuda\", torch_dtype=\"auto\",\n",
    "                                                         offload_folder=\"offload_folder\", trust_remote_code=True)\n",
    "\n",
    "            #model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True)\n",
    "        elif model_index == 1:\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True)\n",
    "        elif model_index == 2:\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True)\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True, download_path = download_path)\n",
    "    if model_index <= 0:\n",
    "        inputs = tokenizer(input, return_tensors=\"pt\")\n",
    "        outputs = model.generate(inputs.input_ids.cuda(), \n",
    "                                 temperature = temperature,\n",
    "                                 top_k = top_k,\n",
    "                                 do_sample = do_sample,\n",
    "                                 max_new_tokens = max_new_tokens\n",
    "                                 )\n",
    "    elif model_index >= 1:\n",
    "        model_inputs = tokenizer(input, return_tensors=\"pt\").to(\"cuda\")\n",
    "        inputs = tokenizer.encode(input, return_tensors=\"pt\")\n",
    "        attention_mask = torch.ones(inputs.shape, dtype = torch.long, device=\"cuda\")\n",
    "        outputs = model.generate(model_inputs.input_ids, \n",
    "                                 temperature = temperature,\n",
    "                                 top_k = top_k,\n",
    "                                 do_sample = do_sample,\n",
    "                                 max_new_tokens = max_new_tokens,\n",
    "                                 attention_mask = attention_mask,\n",
    "                                 pad_token_id = tokenizer.eos_token_id,\n",
    "                                 )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if print == True:\n",
    "        print(response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d58ff232-d0a1-4ad3-bab0-b61a5afeb1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_LLM(message, model_index):\n",
    "    message = message + \"\\nYour answer:\\n###\\n\"\n",
    "    str_return = Yi_text_generation(message, model_index = model_index, print = False)\n",
    "    str_return = str_return[str_return.index(\"###\") + 3:]\n",
    "    return str_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edec21ae-81cf-4740-829f-be1fbc4a5347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readExtractionPrompt():\n",
    "    fileName = \"C:\\\\Users\\Administrator\\\\Desktop\\\\3_opensource_llm\\\\extractionPromptPro.txt\"\n",
    "    f = open(fileName, \"r\")\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "    prompt = \"\"\n",
    "    for line in lines:\n",
    "        prompt += line    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6cd6f84-a07d-417d-8871-3a3509f1ab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gptProcessReport_llm(rootPath,targetPath,model_index):\n",
    "    start_time = time.time()\n",
    "    xml_files = []\n",
    "    for dirpath, dirnames, filenames in os.walk(rootPath):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.xml'):\n",
    "                xml_files.append(os.path.join(dirpath, filename))\n",
    "    \n",
    "    for xml_file in xml_files:\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        case_id = root.attrib['CaseID']\n",
    "        summary = root.find('.//SUMMARY')  # 使用 find() 查找 SUMMARY 标签\n",
    "        # 获取 SUMMARY 标签的文本内容\n",
    "        description = summary.text.strip() if summary is not None else \"No summary found\"\n",
    "        print(case_id)\n",
    "\n",
    "        prompt = \"You should help me process a car accident description. You should analyse each sentence in the description. Once you found a sentence that contains impact actions, then drop all the sentences after.\" + \\\n",
    "             \"Output the processed description.\" + \\\n",
    "             \"The accident description is : \" + description\n",
    "        prompt_1 = readExtractionPrompt() + prompt\n",
    "        response = test_LLM(prompt_1,model_index)\n",
    "        print(response)\n",
    "        # print(pres)\n",
    "        try:\n",
    "            filepath = f\"{targetPath}/{case_id}.txt\"\n",
    "            f = open(filepath, \"w\")\n",
    "            f.write(response)\n",
    "        except:\n",
    "            print('dont write txt')\n",
    "    end_time = time.time()\n",
    "    print(\"Time used:\", end_time - start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7306f22-ff66-4744-8ddc-7d10ea3bca43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2005005289123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "2005002585683\n",
      "\n",
      "\n",
      "2005045587801\n",
      "\n",
      "\n",
      "2005004112761\n",
      "\n",
      "\n",
      "2005006445022\n",
      "\n",
      "\n",
      "200501269400\n",
      "\n",
      "\n",
      "2005004112521\n",
      "\n",
      "import json\n",
      "import re\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "from PIL import Image\n",
      "from collections import Counter\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.manifold import TSNE\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.decomposition import TruncatedSVD\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.metrics import silhouette_score\n",
      "from sklearn.metrics import silhouette_samples\n",
      "from sklearn.metrics import silhouette_score\n",
      "from sklearn.metrics import silhouette_samples\n",
      "from sklearn.metrics import silhouette_samples\n",
      "from sklearn.metrics import silhouette_samples\n",
      "from sklearn.metrics import silhouette_samples\n",
      "from sklearn.metrics import silhouette_samples\n",
      "from sklearn.metrics import silhouette_samples\n",
      "from sklearn.metrics import silhouette_samples\n",
      "from sklearn.metrics import silhouette_samples\n",
      "from sklearn.metrics import silhouette_samples\n",
      "from sklearn.metrics import silhouette_samples\n",
      "from sklearn.metrics import silhouette_samples\n",
      "from sklearn.metrics import silhouette_samples\n",
      "from sklearn.metrics import silhouette_samples\n",
      "from sklearn.metrics import silhouette_samples\n",
      "from sklearn.metrics import\n",
      "2005078436824\n",
      "\n",
      "\n",
      "2005002229602\n",
      "\n",
      "\n",
      "2005002229542\n",
      "\n",
      "\n",
      "2005075585601\n",
      "\n",
      "\n",
      "2005002229442\n",
      "\n",
      "\n",
      "2005075585221\n",
      "\n",
      "import re\n",
      "import json\n",
      "import numpy as np\n",
      "from collections import Counter\n",
      "from collections import defaultdict\n",
      "\n",
      "def get_weather(text):\n",
      "    weather = []\n",
      "    if'sunny' in text:\n",
      "        weather.append('sunny')\n",
      "    if 'clear' in text:\n",
      "        weather.append('clear')\n",
      "    if 'rainy' in text:\n",
      "        weather.append('rainy')\n",
      "    if 'foggy' in text:\n",
      "        weather.append('foggy')\n",
      "    if'snowy' in text:\n",
      "        weather.append('snowy')\n",
      "    if 'windy' in text:\n",
      "        weather.append('windy')\n",
      "    if 'cloudy' in text:\n",
      "        weather.append('cloudy')\n",
      "    if not weather:\n",
      "        weather.append('unknown')\n",
      "    return weather\n",
      "\n",
      "def get_lighting(text):\n",
      "    lighting = []\n",
      "    if 'bright' in text:\n",
      "        lighting.append('bright')\n",
      "    if 'normal' in text:\n",
      "        lighting.append('normal')\n",
      "    if 'dark' in text:\n",
      "        lighting.append('dark')\n",
      "    if 'dusk' in text:\n",
      "        lighting.append('dark')\n",
      "    if 'night' in\n",
      "2005005289642\n",
      "\n",
      "\n",
      "2006002229448\n",
      "\n",
      "\n",
      "2005004112402\n",
      "\n",
      "\n",
      "2005004112862\n",
      "\n",
      "import json\n",
      "import re\n",
      "import string\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "from collections import Counter\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from nltk import pos_tag\n",
      "from nltk import word_tokenize\n",
      "from nltk import sent_tokenize\n",
      "from nltk import ne_chunk\n",
      "from nltk import pos_tag\n",
      "from nltk import word_tokenize\n",
      "from nltk import ne_chunk\n",
      "from nltk import pos_tag\n",
      "from nltk import word_tokenize\n",
      "from nltk import ne_chunk\n",
      "from nltk import pos_tag\n",
      "from nltk import word_tokenize\n",
      "from nltk import ne_chunk\n",
      "from nltk import pos_tag\n",
      "from nltk import word_tokenize\n",
      "from nltk import ne_chunk\n",
      "from nltk import pos_tag\n",
      "from nltk import word_tokenize\n",
      "from nltk import ne_chunk\n",
      "from nltk import pos_tag\n",
      "from nltk import word_tokenize\n",
      "from nltk import ne_chunk\n",
      "from nltk import pos_tag\n",
      "from nltk import word_tokenize\n",
      "from nltk import ne_chunk\n",
      "from nltk import pos_tag\n",
      "from nltk import word_tokenize\n",
      "from nltk import\n",
      "2005002229021\n",
      "\n",
      "\n",
      "2005005289822\n",
      "\n",
      "\n",
      "2005003498482\n",
      "\n",
      "import json\n",
      "import re\n",
      "from collections import Counter\n",
      "\n",
      "def get_weather(line):\n",
      "    weather = []\n",
      "    if'sunny' in line:\n",
      "        weather.append('sunny')\n",
      "    elif 'clear' in line:\n",
      "        weather.append('clear')\n",
      "    elif 'rainy' in line:\n",
      "        weather.append('rainy')\n",
      "    elif 'foggy' in line:\n",
      "        weather.append('foggy')\n",
      "    elif'snowy' in line:\n",
      "        weather.append('snowy')\n",
      "    elif 'windy' in line:\n",
      "        weather.append('windy')\n",
      "    elif 'cloudy' in line:\n",
      "        weather.append('cloudy')\n",
      "    else:\n",
      "        weather.append('unknown')\n",
      "    return weather\n",
      "\n",
      "def get_lighting(line):\n",
      "    lighting = []\n",
      "    if 'bright' in line:\n",
      "        lighting.append('bright')\n",
      "    elif 'normal' in line:\n",
      "        lighting.append('normal')\n",
      "    elif 'dark' in line:\n",
      "        lighting.append('dark')\n",
      "    elif 'dusk' in line:\n",
      "        lighting.append('dark')\n",
      "    elif 'night' in line:\n",
      "        lighting.append('dark')\n",
      "    elif 'light\n",
      "2005004112021\n",
      "\n",
      "import json\n",
      "import re\n",
      "from collections import defaultdict\n",
      "\n",
      "def get_weather(text):\n",
      "    weather = []\n",
      "    if'sunny' in text:\n",
      "        weather.append('sunny')\n",
      "    elif 'clear' in text:\n",
      "        weather.append('clear')\n",
      "    elif 'rainy' in text:\n",
      "        weather.append('rainy')\n",
      "    elif 'foggy' in text:\n",
      "        weather.append('foggy')\n",
      "    elif'snowy' in text:\n",
      "        weather.append('snowy')\n",
      "    elif 'windy' in text:\n",
      "        weather.append('windy')\n",
      "    elif 'cloudy' in text:\n",
      "        weather.append('cloudy')\n",
      "    else:\n",
      "        weather.append('unknown')\n",
      "    return weather\n",
      "\n",
      "def get_lighting(text):\n",
      "    lighting = []\n",
      "    if 'bright' in text:\n",
      "        lighting.append('bright')\n",
      "    elif 'normal' in text:\n",
      "        lighting.append('normal')\n",
      "    elif 'dark' in text:\n",
      "        lighting.append('dark')\n",
      "    elif 'dusk' in text:\n",
      "        lighting.append('dark')\n",
      "    elif 'night' in text:\n",
      "        lighting.append('dark')\n",
      "    elif 'light\n",
      "2005005447262\n",
      "\n",
      "import json\n",
      "import re\n",
      "from collections import defaultdict\n",
      "from typing import List, Dict, Any, Tuple\n",
      "\n",
      "def process_description(description: str) -> Dict[str, Any]:\n",
      "    \"\"\"\n",
      "    Process a car accident description.\n",
      "    You should analyse each sentence in the description.\n",
      "    Once you found a sentence that contains impact actions, then drop all the sentences after.\n",
      "    Output the processed description.\n",
      "    \"\"\"\n",
      "    # 1. split the description into sentences\n",
      "    sentences = description.split(\".\")\n",
      "    # 2. extract the impact actions from each sentence\n",
      "    impact_actions = extract_impact_actions(sentences)\n",
      "    # 3. process the impact actions\n",
      "    processed_impact_actions = process_impact_actions(impact_actions)\n",
      "    # 4. output the processed description\n",
      "    return output_processed_description(processed_impact_actions)\n",
      "\n",
      "def extract_impact_actions(sentences: List[str]) -> List[str]:\n",
      "    \"\"\"\n",
      "    Extract the impact actions from each sentence.\n",
      "    \"\"\"\n",
      "    impact_actions = []\n",
      "    for sentence in sentences:\n",
      "        # 1. split the sentence into words\n",
      "        words = sentence.split()\n",
      "        # 2. check if\n",
      "2005045706121\n",
      "\n",
      "\n",
      "2005004112881\n",
      "\n",
      "\n",
      "2005005289162\n",
      "\n",
      "\n",
      "2005002229282\n",
      "\n",
      "\n",
      "2005002229042\n",
      "\n",
      "\n",
      "2005045587921\n",
      "\n",
      "\n",
      "2005072709641\n",
      "\n",
      "\n",
      "2005003588201\n",
      "\n",
      "\n",
      "2005004112182\n",
      "\n",
      "\n",
      "2005076197061\n",
      "\n",
      "\n",
      "2005002229563\n",
      "\n",
      "\n",
      "2005012695903\n",
      "\n",
      "\n",
      "2005004495863\n",
      "\n",
      "\n",
      "2005003498501\n",
      "\n",
      "\n",
      "2005045588442\n",
      "\n",
      "\n",
      "2005003498481\n",
      "\n",
      "\n",
      "2005005447482\n",
      "\n",
      "\n",
      "2005004112601\n",
      "\n",
      "\n",
      "2005004495301\n",
      "\n",
      "\n",
      "2005043601782\n",
      "\n",
      "\n",
      "2005002229482\n",
      "\n",
      "\n",
      "2005004112561\n",
      "\n",
      "\n",
      "2005008450282\n",
      "\n",
      "\n",
      "2005004112721\n",
      "\n",
      "\n",
      "2005005289222\n",
      "\n",
      "\n",
      "2005002585502\n",
      "\n",
      "\n",
      "2005006445302\n",
      "\n",
      "\n",
      "2005003498503\n",
      "\n",
      "\n",
      "Time used: 191.06779289245605\n"
     ]
    }
   ],
   "source": [
    "gptProcessReport_llm(\"C:\\\\Users\\\\Administrator\\\\Desktop\\\\3_opensource_llm\\\\raw_reports\",\"C:\\\\Users\\\\Administrator\\\\Desktop\\\\3_opensource_llm\\\\llama_3_2_3b\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0851aef-7b01-4188-af8f-72bc10074a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n",
      "12.1\n",
      "0\n",
      "True\n",
      "1.21.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.is_available())\n",
    "import modelscope\n",
    "print(modelscope.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfb1aca-82a2-4b67-9dcb-d7c360efe500",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Phi-3-mini",
   "language": "python",
   "name": "phi-3-mini"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
